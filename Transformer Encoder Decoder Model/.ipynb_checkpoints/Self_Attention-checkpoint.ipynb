{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L - Length of the input sequence \n",
    "# d_k,d_v - Size of each of the vector for illustrative purpose it's 8\n",
    "L,d_k,d_v = 4,8,8\n",
    "q = np.random.randn(L,d_k)\n",
    "k = np.random.randn(L,d_k)\n",
    "v = np.random.randn(L,d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.44008364 -0.65893923 -0.82743603  2.07134499 -0.756638    0.18606377\n",
      "  -2.52939241 -0.99671064]\n",
      " [ 0.45847306  0.13226699  0.39716341 -0.35557593  0.92675916  0.86731148\n",
      "  -1.0848844  -0.75466212]\n",
      " [-1.73679108  0.50542825  0.67550296 -0.48343931 -0.78572117 -0.57689201\n",
      "  -0.52532605 -1.41217445]\n",
      " [-2.09387521 -0.89363413 -0.59569919 -1.71644659 -0.36390696 -0.30201351\n",
      "   0.22337068 -0.50049467]]\n",
      "K\n",
      " [[-0.97577445  0.8762965  -2.38211946 -1.7001989  -2.1416398   0.14183991\n",
      "   0.82795221  0.06947477]\n",
      " [ 0.75104729  0.51849052  0.78832873  0.55552962  1.34575561  0.47145897\n",
      "   0.51505236  1.26781612]\n",
      " [-0.43040316  1.29402735 -0.45535669 -0.2645018   0.40588437 -0.22652967\n",
      "   0.51160386 -1.89667442]\n",
      " [ 0.13683429 -1.27318423 -0.18401422  1.90297292 -0.61696554  1.93138142\n",
      "  -0.68380964  0.23601658]]\n",
      "V\n",
      " [[ 1.5062644  -1.35215475  0.48031349 -0.28786724 -0.40148487  2.23991922\n",
      "  -2.90143267 -1.0343477 ]\n",
      " [ 1.38140753 -0.31255013  1.31883032 -0.84232214 -2.56852263  0.81663448\n",
      "   1.73925434  1.34325108]\n",
      " [-0.26513094  0.00681401 -0.9914591  -1.62039073 -0.74181731  0.87437046\n",
      "  -0.64967023 -0.29810682]\n",
      " [ 0.80614759  1.30841639  0.46847401  0.03525922  1.79016125 -1.30955226\n",
      "   0.35361817 -0.03593953]]\n"
     ]
    }
   ],
   "source": [
    "# Using the randn function - values are assigned from the normal distribution \n",
    "print(\"Q\\n\",q) # 8 x 1 - for Every single word - Totally 8 x 4\n",
    "print(\"K\\n\",k) # 8 x 1 - for Every single word - Totally 8 x 4\n",
    "print(\"V\\n\",v) # 8 x 1 - for Every single word - Totally 8 x 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the Formula of Self Attention - Refer to the Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.07412046, -3.00967196, -0.96606258,  7.31370534],\n",
       "       [-3.48542943,  0.66902454,  0.94303088,  0.81167197],\n",
       "       [ 2.41828564, -4.16871647,  3.44328164, -2.5289391 ],\n",
       "       [ 6.48407724, -4.61068825,  1.45434592, -2.93514059]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Self Attention - Initially from the encoder side we need all the words to look at other words so that we can check the similarity/affinity with each other\n",
    "# Coming to the First Step \n",
    "np.matmul(q,k.T)\n",
    "# for each row we have to look at the max value - that's the one each row will focus on \n",
    "# my - this will focus on the 3rd index\n",
    "# name - this will focus on the 2nd index\n",
    "# is - this will focus on the 2nd index \n",
    "# Vibhav - this will focus on the 1st index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8851675882015266, 1.1668235844412789, 12.65558917253314)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why do we need the denominator - SquareRoot(d_k)\n",
    "# Answer - In order to minimize the variance and hence stablize the values of the multiplication(q,k.T) vector\n",
    "q.var(), k.var(), np.matmul(q,k.T).var()\n",
    "# As you can see below the difference of the first two values is comparitively less \n",
    "# Whereas the last value is far way distant from the first two values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8851675882015266, 1.1668235844412789, 1.581948646566643)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to make sure we stablize the values and we reduce the variance - we use the SquareRoot(dk)\n",
    "scaled = np.matmul(q,k.T)/math.sqrt(d_k)\n",
    "q.var(),k.var(),scaled.var()\n",
    "# As you can see now the variance is much more in the same range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.08686571, -1.06407973, -0.3415547 ,  2.58578532],\n",
       "       [-1.23228539,  0.2365359 ,  0.33341177,  0.28696938],\n",
       "       [ 0.85499309, -1.47386384,  1.2173839 , -0.89411499],\n",
       "       [ 2.29246749, -1.63012447,  0.51418893, -1.03772891]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now you can see the scaled vector will have much more of lower values - which will be of the same range \n",
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking \n",
    "* This is required specifically in the decoding part of the transformer - so that we don't look at a future word while generating a current word \n",
    "* For the **Encoder** Masking is not required - **Cause all the inputs are passed simultaneously**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will generate a lower triangular matrix\n",
    "mask = np.tril(np.ones((L,L)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask==0] = -np.infty\n",
    "mask[mask==1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform every single 0 to -inf\n",
    "# Transform every single 1 to 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.08686571,        -inf,        -inf,        -inf],\n",
       "       [-1.23228539,  0.2365359 ,        -inf,        -inf],\n",
       "       [ 0.85499309, -1.47386384,  1.2173839 ,        -inf],\n",
       "       [ 2.29246749, -1.63012447,  0.51418893, -1.03772891]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# why 0 - because we are adding so that if so the value is 0, then we will get the exact values from the scaled to the masked one - there will not be any change in the values\n",
    "scaled + mask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax - Refer the notes for the formula\n",
    "* This is used to convert a vector into a probability distribution\n",
    "* So that their values add upto 1 - they are very interpretable and also very stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  return (np.exp(x).T/np.sum(np.exp(x),axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = softmax(scaled+mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.18712184, 0.81287816, 0.        , 0.        ],\n",
       "       [0.39460693, 0.03843749, 0.56695558, 0.        ],\n",
       "       [0.81665685, 0.01616142, 0.13795678, 0.02922494]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each row will add upto 1 here for this case\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02299578, 0.02352577, 0.04845432, 0.90502413],\n",
       "       [0.06803211, 0.29553909, 0.32560239, 0.31082641],\n",
       "       [0.36926308, 0.03596882, 0.53054254, 0.06422556],\n",
       "       [0.81665685, 0.01616142, 0.13795678, 0.02922494]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't apply the mask then - every row is going to add upto 1 - Cause it is a probability distribution\n",
    "attention_new = softmax(scaled)\n",
    "attention_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.5062644 , -1.35215475,  0.48031349, -0.28786724, -0.40148487,\n",
       "         2.23991922, -2.90143267, -1.0343477 ],\n",
       "       [ 1.40477097, -0.50708286,  1.16192551, -0.73857152, -2.16302254,\n",
       "         1.08296214,  0.87088045,  0.89835042],\n",
       "       [ 0.49716274, -0.54172004, -0.32188572, -1.06466073, -0.67773373,\n",
       "         1.41100624, -1.44640703, -0.5255429 ],\n",
       "       [ 1.23940967, -1.07011926,  0.2904781 , -0.47121531, -0.41940771,\n",
       "         1.9247971 , -2.420658  , -0.86517448]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atlast we multiply it with the v vector - This is After Attention\n",
    "new_v = np.matmul(attention,v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.5062644 , -1.35215475,  0.48031349, -0.28786724, -0.40148487,\n",
       "         2.23991922, -2.90143267, -1.0343477 ],\n",
       "       [ 1.38140753, -0.31255013,  1.31883032, -0.84232214, -2.56852263,\n",
       "         0.81663448,  1.73925434,  1.34325108],\n",
       "       [-0.26513094,  0.00681401, -0.9914591 , -1.62039073, -0.74181731,\n",
       "         0.87437046, -0.64967023, -0.29810682],\n",
       "       [ 0.80614759,  1.30841639,  0.46847401,  0.03525922,  1.79016125,\n",
       "        -1.30955226,  0.35361817, -0.03593953]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before Attention\n",
    "# here the first vector will be kind of similar cause all of them are 0 except the first one\n",
    "# As you go to the late words we get to know how the vectors have actually become\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining all the code together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  return (np.exp(x).T/np.sum(np.exp(x),axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q,k,v,mask=None):\n",
    "  d_k = q.shape[-1] # We have 4 rows and 8 columns so it is (4,8) - this is the shape and it retrieves -1 which is 8 here\n",
    "  scaled = np.matmul(q,k.T)/math.sqrt(d_k)\n",
    "  if mask is not None:\n",
    "    scaled = scaled + mask\n",
    "  attention = softmax(scaled)\n",
    "  out = np.matmul(attention,v)\n",
    "  return out,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.44008364 -0.65893923 -0.82743603  2.07134499 -0.756638    0.18606377\n",
      "  -2.52939241 -0.99671064]\n",
      " [ 0.45847306  0.13226699  0.39716341 -0.35557593  0.92675916  0.86731148\n",
      "  -1.0848844  -0.75466212]\n",
      " [-1.73679108  0.50542825  0.67550296 -0.48343931 -0.78572117 -0.57689201\n",
      "  -0.52532605 -1.41217445]\n",
      " [-2.09387521 -0.89363413 -0.59569919 -1.71644659 -0.36390696 -0.30201351\n",
      "   0.22337068 -0.50049467]]\n",
      "K\n",
      " [[-0.97577445  0.8762965  -2.38211946 -1.7001989  -2.1416398   0.14183991\n",
      "   0.82795221  0.06947477]\n",
      " [ 0.75104729  0.51849052  0.78832873  0.55552962  1.34575561  0.47145897\n",
      "   0.51505236  1.26781612]\n",
      " [-0.43040316  1.29402735 -0.45535669 -0.2645018   0.40588437 -0.22652967\n",
      "   0.51160386 -1.89667442]\n",
      " [ 0.13683429 -1.27318423 -0.18401422  1.90297292 -0.61696554  1.93138142\n",
      "  -0.68380964  0.23601658]]\n",
      "V\n",
      " [[ 1.5062644  -1.35215475  0.48031349 -0.28786724 -0.40148487  2.23991922\n",
      "  -2.90143267 -1.0343477 ]\n",
      " [ 1.38140753 -0.31255013  1.31883032 -0.84232214 -2.56852263  0.81663448\n",
      "   1.73925434  1.34325108]\n",
      " [-0.26513094  0.00681401 -0.9914591  -1.62039073 -0.74181731  0.87437046\n",
      "  -0.64967023 -0.29810682]\n",
      " [ 0.80614759  1.30841639  0.46847401  0.03525922  1.79016125 -1.30955226\n",
      "   0.35361817 -0.03593953]]\n",
      "New V\n",
      " [[ 0.78387267  1.14603174  0.41801148 -0.0730405   1.51453592 -1.07208872\n",
      "   0.26275025 -0.03915531]\n",
      " [ 0.67497896  0.2245483   0.24523529 -0.78516693 -0.4715208   0.27138753\n",
      "   0.21500674  0.21837909]\n",
      " [ 0.51700746 -0.42289399 -0.27112442 -0.99401775 -0.5192318   1.23627684\n",
      "  -1.33079941 -0.49409785]\n",
      " [ 1.23940967 -1.07011926  0.2904781  -0.47121531 -0.41940771  1.9247971\n",
      "  -2.420658   -0.86517448]]\n",
      "Attention\n",
      " [[0.02299578 0.02352577 0.04845432 0.90502413]\n",
      " [0.06803211 0.29553909 0.32560239 0.31082641]\n",
      " [0.36926308 0.03596882 0.53054254 0.06422556]\n",
      " [0.81665685 0.01616142 0.13795678 0.02922494]]\n"
     ]
    }
   ],
   "source": [
    "values,attention = scaled_dot_product_attention(q,k,v,mask=None)\n",
    "print(\"Q\\n\",q) \n",
    "print(\"K\\n\",k)\n",
    "print(\"V\\n\",v)\n",
    "print(\"New V\\n\",values)\n",
    "print(\"Attention\\n\",attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
