{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L - Length of the input sequence \n",
    "# d_k,d_v - Size of each of the vector for illustrative purpose it's 8\n",
    "L,d_k,d_v = 4,8,8\n",
    "q = np.random.randn(L,d_k)\n",
    "k = np.random.randn(L,d_k)\n",
    "v = np.random.randn(L,d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.44008364 -0.65893923 -0.82743603  2.07134499 -0.756638    0.18606377\n",
      "  -2.52939241 -0.99671064]\n",
      " [ 0.45847306  0.13226699  0.39716341 -0.35557593  0.92675916  0.86731148\n",
      "  -1.0848844  -0.75466212]\n",
      " [-1.73679108  0.50542825  0.67550296 -0.48343931 -0.78572117 -0.57689201\n",
      "  -0.52532605 -1.41217445]\n",
      " [-2.09387521 -0.89363413 -0.59569919 -1.71644659 -0.36390696 -0.30201351\n",
      "   0.22337068 -0.50049467]]\n",
      "K\n",
      " [[-0.97577445  0.8762965  -2.38211946 -1.7001989  -2.1416398   0.14183991\n",
      "   0.82795221  0.06947477]\n",
      " [ 0.75104729  0.51849052  0.78832873  0.55552962  1.34575561  0.47145897\n",
      "   0.51505236  1.26781612]\n",
      " [-0.43040316  1.29402735 -0.45535669 -0.2645018   0.40588437 -0.22652967\n",
      "   0.51160386 -1.89667442]\n",
      " [ 0.13683429 -1.27318423 -0.18401422  1.90297292 -0.61696554  1.93138142\n",
      "  -0.68380964  0.23601658]]\n",
      "V\n",
      " [[ 1.5062644  -1.35215475  0.48031349 -0.28786724 -0.40148487  2.23991922\n",
      "  -2.90143267 -1.0343477 ]\n",
      " [ 1.38140753 -0.31255013  1.31883032 -0.84232214 -2.56852263  0.81663448\n",
      "   1.73925434  1.34325108]\n",
      " [-0.26513094  0.00681401 -0.9914591  -1.62039073 -0.74181731  0.87437046\n",
      "  -0.64967023 -0.29810682]\n",
      " [ 0.80614759  1.30841639  0.46847401  0.03525922  1.79016125 -1.30955226\n",
      "   0.35361817 -0.03593953]]\n"
     ]
    }
   ],
   "source": [
    "# Using the randn function - values are assigned from the normal distribution \n",
    "print(\"Q\\n\",q) # 8 x 1 - for Every single word - Totally 8 x 4\n",
    "print(\"K\\n\",k) # 8 x 1 - for Every single word - Totally 8 x 4\n",
    "print(\"V\\n\",v) # 8 x 1 - for Every single word - Totally 8 x 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the Formula of Self Attention - Refer to the Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.07412046, -3.00967196, -0.96606258,  7.31370534],\n",
       "       [-3.48542943,  0.66902454,  0.94303088,  0.81167197],\n",
       "       [ 2.41828564, -4.16871647,  3.44328164, -2.5289391 ],\n",
       "       [ 6.48407724, -4.61068825,  1.45434592, -2.93514059]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Self Attention - Initially from the encoder side we need all the words to look at other words so that we can check the similarity/affinity with each other\n",
    "# Coming to the First Step \n",
    "np.matmul(q,k.T)\n",
    "# for each row we have to look at the max value - that's the one each row will focus on \n",
    "# my - this will focus on the 3rd index\n",
    "# name - this will focus on the 2nd index\n",
    "# is - this will focus on the 2nd index \n",
    "# Vibhav - this will focus on the 1st index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8851675882015266, 1.1668235844412789, 12.65558917253314)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why do we need the denominator - SquareRoot(d_k)\n",
    "# Answer - In order to minimize the variance and hence stablize the values of the multiplication(q,k.T) vector\n",
    "q.var(), k.var(), np.matmul(q,k.T).var()\n",
    "# As you can see below the difference of the first two values is comparitively less \n",
    "# Whereas the last value is far way distant from the first two values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8851675882015266, 1.1668235844412789, 1.581948646566643)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to make sure we stablize the values and we reduce the variance - we use the SquareRoot(dk)\n",
    "scaled = np.matmul(q,k.T)/math.sqrt(d_k)\n",
    "q.var(),k.var(),scaled.var()\n",
    "# As you can see now the variance is much more in the same range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.08686571, -1.06407973, -0.3415547 ,  2.58578532],\n",
       "       [-1.23228539,  0.2365359 ,  0.33341177,  0.28696938],\n",
       "       [ 0.85499309, -1.47386384,  1.2173839 , -0.89411499],\n",
       "       [ 2.29246749, -1.63012447,  0.51418893, -1.03772891]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now you can see the scaled vector will have much more of lower values - which will be of the same range \n",
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking \n",
    "* This is required specifically in the decoding part of the transformer - so that we don't look at a future word while generating a current word \n",
    "* For the **Encoder** Masking is not required - **Cause all the inputs are passed simultaneously**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will generate a lower triangular matrix\n",
    "mask = np.tril(np.ones((L,L)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask==0] = -np.infty\n",
    "mask[mask==1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform every single 0 to -inf\n",
    "# Transform every single 1 to 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.08686571,        -inf,        -inf,        -inf],\n",
       "       [-1.23228539,  0.2365359 ,        -inf,        -inf],\n",
       "       [ 0.85499309, -1.47386384,  1.2173839 ,        -inf],\n",
       "       [ 2.29246749, -1.63012447,  0.51418893, -1.03772891]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# why 0 - because we are adding so that if so the value is 0, then we will get the exact values from the scaled to the masked one - there will not be any change in the values\n",
    "scaled + mask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax \n",
    "* This is used to convert a vector into a probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
