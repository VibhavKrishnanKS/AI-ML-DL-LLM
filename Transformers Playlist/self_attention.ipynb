{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here \n",
    "# L - Length of the input sequence (my name is vibhav)\n",
    "# 8 - This is the size of each of the vector\n",
    "# Every one will be of size - (4x8)\n",
    "L, d_k, d_v = 4,8,8\n",
    "# query vector\n",
    "q = np.random.randn(L, d_k)\n",
    "# key vector\n",
    "k = np.random.randn(L, d_k)\n",
    "# value vector\n",
    "v = np.random.randn(L, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 1.09761055  0.25431486 -0.64855585 -1.1783437  -1.78496968 -0.91920149\n",
      "  -0.24095985 -1.04872552]\n",
      " [-0.08069951  0.12154865  0.54870725 -0.1662838  -1.13616968 -0.38030652\n",
      "   0.92407845  1.88154461]\n",
      " [ 0.12960108  0.9027782   1.62437549  1.25708041  2.22449749 -1.30507629\n",
      "   1.46377707 -1.0209322 ]\n",
      " [ 2.22076171  0.45139842 -1.52001994 -1.65998052  0.71400261 -0.54140543\n",
      "  -0.62444227 -3.20534793]]\n",
      "K\n",
      " [[-0.21165531 -0.73893031 -0.85166574 -0.2933164   0.45192866 -0.83067114\n",
      "  -1.56054348 -0.64977261]\n",
      " [-0.54152624  2.02183667 -2.1343795   0.00595999  1.08572903 -2.48480572\n",
      "   1.38222672 -0.33252892]\n",
      " [ 1.21186875 -1.07273332 -0.75972709  1.00311054 -0.94099936  0.74606439\n",
      "   0.48767779 -0.62625095]\n",
      " [ 0.1311618  -1.03994281 -0.05969253 -0.33845304  1.84588101  1.84488924\n",
      "  -0.54124462 -0.08792945]]\n",
      "V\n",
      " [[ 0.80998809  0.46072378  0.84466935  0.15256506 -0.01857584  0.51588124\n",
      "   0.56995535 -0.11847627]\n",
      " [ 0.79882088 -0.52920189 -0.72286384 -0.43575666 -0.5270247  -0.81931937\n",
      "  -0.47483298  0.30671602]\n",
      " [-1.12203332 -0.09181229 -1.72404342 -0.0122756  -0.97529801 -0.29117605\n",
      "  -0.03585067  0.76873726]\n",
      " [-1.00187918  0.50329083  0.66869131 -0.42007772 -0.69212416  0.48800077\n",
      "  -0.82816828  0.39039161]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\",q)\n",
    "print(\"K\\n\",k)\n",
    "print(\"V\\n\",v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.49208091,  1.65875372,  1.90119079, -4.45101487],\n",
       "       [-3.35347564, -0.5196548 , -0.75412008, -3.57791591],\n",
       "       [-1.9781788 ,  6.31636994, -2.49818255, -0.44831974],\n",
       "       [ 4.80748514,  5.26770984,  2.32372421,  1.41336116]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to create a SELF ATTENTION MATRIX, we needd every word to look at every single other word - To see whether it has higher affinity towards it or not either by using any of the vector embedding technique\n",
    "# The above concept is represented by the Query(Q) - what i am looking for ?\n",
    "np.matmul(q,k.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.591199848655795, 1.1749383373971596, 9.886851083364917)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# why do we need sqrt(d_k) in the denominator - In order to minimize the variance\n",
    "q.var(), k.var(), np.matmul(q,k.T).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.591199848655795, 1.1749383373971596, 1.2358563854206144)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q,k.T)/math.sqrt(d_k)\n",
    "q.var(),k.var(),scaled.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52753027,  0.586458  ,  0.67217245, -1.5736714 ],\n",
       "       [-1.18563268, -0.18372572, -0.26662171, -1.2649843 ],\n",
       "       [-0.69939182,  2.23317401, -0.88324091, -0.15850496],\n",
       "       [ 1.69970267,  1.86241668,  0.82156057,  0.49969863]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking\n",
    "* This is to ensure words don't get context from words generate in the future\n",
    "* This concept won't be used in the encoders, but this will be helpful in the decoders\n",
    "* So that we don't look at a future word while generating the current context\n",
    "### Why it is not required for encoders\n",
    "* As our inputs are passed simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why are we doing this so that \n",
    "# my can look at only itself \n",
    "# name can look at my name\n",
    "# is can look at my name is\n",
    "# vibhav can look at my name is vibhav\n",
    "mask = np.tril(np.ones((L,L)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52753027,        -inf,        -inf,        -inf],\n",
       "       [-1.18563268, -0.18372572,        -inf,        -inf],\n",
       "       [-0.69939182,  2.23317401, -0.88324091,        -inf],\n",
       "       [ 1.69970267,  1.86241668,  0.82156057,  0.49969863]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anyways we are not going to consider any context from it - so we replace them to be (-inf)\n",
    "scaled + mask\n",
    "# This is because of the softmax operation which we will be going to perform next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "* This operation is used to convert vector into a prob distribution - so that their values add up to 1 and they are interpretable and stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  return (np.exp(x).T/np.sum(np.exp(x),axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2995237 , 0.31770437, 0.34613738, 0.03663455],\n",
       "       [0.13978203, 0.38069223, 0.35040697, 0.12911877],\n",
       "       [0.04479216, 0.84100616, 0.03726983, 0.07693185],\n",
       "       [0.3456085 , 0.40667756, 0.14361908, 0.10409486]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This shouldn't be the case - because each row adds to 1\n",
    "attention_diff = softmax(scaled)\n",
    "attention_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.26856665, 0.73143335, 0.        , 0.        ],\n",
       "       [0.0485253 , 0.91109867, 0.04037603, 0.        ],\n",
       "       [0.3456085 , 0.40667756, 0.14361908, 0.10409486]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the right method - cause as you can see for the first row it is 1.\n",
    "# similarly for the next row first two columns add to 1\n",
    "# similarly for the next row first three columns add to 1\n",
    "# for the last row all the 4 columns add to 1\n",
    "attention = softmax(scaled + mask)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.80998809,  0.46072378,  0.84466935,  0.15256506, -0.01857584,\n",
       "         0.51588124,  0.56995535, -0.11847627],\n",
       "       [ 0.80182002, -0.26334086, -0.3018767 , -0.27775306, -0.39047229,\n",
       "        -0.46072901, -0.19423768,  0.19252355],\n",
       "       [ 0.72180631, -0.46350539, -0.68722248, -0.39010969, -0.52045157,\n",
       "        -0.73320403, -0.40640995,  0.30473801],\n",
       "       [ 0.33936543, -0.01678049, -0.1800458 , -0.16997562, -0.43286706,\n",
       "        -0.14592593, -0.08747941,  0.23483121]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention, v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.80998809,  0.46072378,  0.84466935,  0.15256506, -0.01857584,\n",
       "         0.51588124,  0.56995535, -0.11847627],\n",
       "       [ 0.79882088, -0.52920189, -0.72286384, -0.43575666, -0.5270247 ,\n",
       "        -0.81931937, -0.47483298,  0.30671602],\n",
       "       [-1.12203332, -0.09181229, -1.72404342, -0.0122756 , -0.97529801,\n",
       "        -0.29117605, -0.03585067,  0.76873726],\n",
       "       [-1.00187918,  0.50329083,  0.66869131, -0.42007772, -0.69212416,\n",
       "         0.48800077, -0.82816828,  0.39039161]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  return (np.exp(x).T/np.sum(np.exp(x),axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q,k,v,mask=None):\n",
    "  d_k = q.shape[-1]\n",
    "  scaled = np.matmul(q,k.T)/math.sqrt(d_k)\n",
    "  if mask is not None:\n",
    "    scaled = scaled  + mask\n",
    "  attention = softmax(scaled)\n",
    "  out = np.matmul(attention,v)\n",
    "  return out,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 1.09761055  0.25431486 -0.64855585 -1.1783437  -1.78496968 -0.91920149\n",
      "  -0.24095985 -1.04872552]\n",
      " [-0.08069951  0.12154865  0.54870725 -0.1662838  -1.13616968 -0.38030652\n",
      "   0.92407845  1.88154461]\n",
      " [ 0.12960108  0.9027782   1.62437549  1.25708041  2.22449749 -1.30507629\n",
      "   1.46377707 -1.0209322 ]\n",
      " [ 2.22076171  0.45139842 -1.52001994 -1.65998052  0.71400261 -0.54140543\n",
      "  -0.62444227 -3.20534793]]\n",
      "K\n",
      " [[-0.21165531 -0.73893031 -0.85166574 -0.2933164   0.45192866 -0.83067114\n",
      "  -1.56054348 -0.64977261]\n",
      " [-0.54152624  2.02183667 -2.1343795   0.00595999  1.08572903 -2.48480572\n",
      "   1.38222672 -0.33252892]\n",
      " [ 1.21186875 -1.07273332 -0.75972709  1.00311054 -0.94099936  0.74606439\n",
      "   0.48767779 -0.62625095]\n",
      " [ 0.1311618  -1.03994281 -0.05969253 -0.33845304  1.84588101  1.84488924\n",
      "  -0.54124462 -0.08792945]]\n",
      "V\n",
      " [[ 0.80998809  0.46072378  0.84466935  0.15256506 -0.01857584  0.51588124\n",
      "   0.56995535 -0.11847627]\n",
      " [ 0.79882088 -0.52920189 -0.72286384 -0.43575666 -0.5270247  -0.81931937\n",
      "  -0.47483298  0.30671602]\n",
      " [-1.12203332 -0.09181229 -1.72404342 -0.0122756  -0.97529801 -0.29117605\n",
      "  -0.03585067  0.76873726]\n",
      " [-1.00187918  0.50329083  0.66869131 -0.42007772 -0.69212416  0.48800077\n",
      "  -0.82816828  0.39039161]]\n",
      "New V\n",
      " [[ 0.07131845 -0.04347389 -0.54891717 -0.11238335 -0.53594471 -0.18869191\n",
      "  -0.02289021  0.34234909]\n",
      " [-0.10520302 -0.10424951 -0.67489529 -0.2031047  -0.63434822 -0.27881765\n",
      "  -0.2205901   0.41998133]\n",
      " [ 0.58919999 -0.38912796 -0.58290951 -0.39241518 -0.53365866 -0.63925448\n",
      "  -0.4388566   0.3113275 ]\n",
      " [ 0.33936543 -0.01678049 -0.1800458  -0.16997562 -0.43286706 -0.14592593\n",
      "  -0.08747941  0.23483121]]\n",
      "Attention\n",
      " [[0.2995237  0.31770437 0.34613738 0.03663455]\n",
      " [0.13978203 0.38069223 0.35040697 0.12911877]\n",
      " [0.04479216 0.84100616 0.03726983 0.07693185]\n",
      " [0.3456085  0.40667756 0.14361908 0.10409486]]\n"
     ]
    }
   ],
   "source": [
    "# This is for the Encoder\n",
    "values,attention = scaled_dot_product_attention(q,k,v,mask=None)\n",
    "print(\"Q\\n\",q)\n",
    "print(\"K\\n\",k)\n",
    "print(\"V\\n\",v)\n",
    "print(\"New V\\n\",values)\n",
    "print(\"Attention\\n\",attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 1.09761055  0.25431486 -0.64855585 -1.1783437  -1.78496968 -0.91920149\n",
      "  -0.24095985 -1.04872552]\n",
      " [-0.08069951  0.12154865  0.54870725 -0.1662838  -1.13616968 -0.38030652\n",
      "   0.92407845  1.88154461]\n",
      " [ 0.12960108  0.9027782   1.62437549  1.25708041  2.22449749 -1.30507629\n",
      "   1.46377707 -1.0209322 ]\n",
      " [ 2.22076171  0.45139842 -1.52001994 -1.65998052  0.71400261 -0.54140543\n",
      "  -0.62444227 -3.20534793]]\n",
      "K\n",
      " [[-0.21165531 -0.73893031 -0.85166574 -0.2933164   0.45192866 -0.83067114\n",
      "  -1.56054348 -0.64977261]\n",
      " [-0.54152624  2.02183667 -2.1343795   0.00595999  1.08572903 -2.48480572\n",
      "   1.38222672 -0.33252892]\n",
      " [ 1.21186875 -1.07273332 -0.75972709  1.00311054 -0.94099936  0.74606439\n",
      "   0.48767779 -0.62625095]\n",
      " [ 0.1311618  -1.03994281 -0.05969253 -0.33845304  1.84588101  1.84488924\n",
      "  -0.54124462 -0.08792945]]\n",
      "V\n",
      " [[ 0.80998809  0.46072378  0.84466935  0.15256506 -0.01857584  0.51588124\n",
      "   0.56995535 -0.11847627]\n",
      " [ 0.79882088 -0.52920189 -0.72286384 -0.43575666 -0.5270247  -0.81931937\n",
      "  -0.47483298  0.30671602]\n",
      " [-1.12203332 -0.09181229 -1.72404342 -0.0122756  -0.97529801 -0.29117605\n",
      "  -0.03585067  0.76873726]\n",
      " [-1.00187918  0.50329083  0.66869131 -0.42007772 -0.69212416  0.48800077\n",
      "  -0.82816828  0.39039161]]\n",
      "New V\n",
      " [[ 0.80998809  0.46072378  0.84466935  0.15256506 -0.01857584  0.51588124\n",
      "   0.56995535 -0.11847627]\n",
      " [ 0.80182002 -0.26334086 -0.3018767  -0.27775306 -0.39047229 -0.46072901\n",
      "  -0.19423768  0.19252355]\n",
      " [ 0.72180631 -0.46350539 -0.68722248 -0.39010969 -0.52045157 -0.73320403\n",
      "  -0.40640995  0.30473801]\n",
      " [ 0.33936543 -0.01678049 -0.1800458  -0.16997562 -0.43286706 -0.14592593\n",
      "  -0.08747941  0.23483121]]\n",
      "Attention\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.26856665 0.73143335 0.         0.        ]\n",
      " [0.0485253  0.91109867 0.04037603 0.        ]\n",
      " [0.3456085  0.40667756 0.14361908 0.10409486]]\n"
     ]
    }
   ],
   "source": [
    "# This is for the Decoder\n",
    "values,attention = scaled_dot_product_attention(q,k,v,mask=mask)\n",
    "print(\"Q\\n\",q)\n",
    "print(\"K\\n\",k)\n",
    "print(\"V\\n\",v)\n",
    "print(\"New V\\n\",values)\n",
    "print(\"Attention\\n\",attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
