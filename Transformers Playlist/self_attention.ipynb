{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here \n",
    "# L - Length of the input sequence (my name is vibhav)\n",
    "# 8 - This is the size of each of the vector\n",
    "# Every one will be of size - (4x8)\n",
    "L, d_k, d_v = 4,8,8\n",
    "# query vector - what am i looking for ?\n",
    "q = np.random.randn(L, d_k)\n",
    "# key vector - what can i offer ?\n",
    "k = np.random.randn(L, d_k)\n",
    "# value vector - what i actually offer?\n",
    "v = np.random.randn(L, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.2751041   0.4958448   1.51066295 -0.14032124 -1.70178345  1.27926355\n",
      "   0.17624542 -0.59921872]\n",
      " [-0.28972129 -0.36650107  0.06336092 -0.77934791  0.03204285 -2.38194955\n",
      "   0.29029974  0.09106863]\n",
      " [-0.33995996  0.65953047  0.83787033 -0.0877142   2.24189994 -1.25214464\n",
      "  -1.11616416  0.3537038 ]\n",
      " [-0.25124162  1.18038437  1.01302541  1.19042449  0.25393742 -0.21432398\n",
      "   0.81623457  0.37074504]]\n",
      "K\n",
      " [[-0.83647365  0.22651162 -0.53878076 -0.08790401 -0.89791675 -0.61861813\n",
      "  -1.21527165  0.22252373]\n",
      " [-1.45747415 -1.1259484  -0.48800815 -0.1256447   0.67235244  0.46567906\n",
      "  -0.558623    0.01859098]\n",
      " [ 1.10837112 -0.8465118  -2.74098825 -1.44178702 -1.36898869 -2.25308163\n",
      "  -1.23695635 -0.39828107]\n",
      " [-1.16720464  0.27535859  0.891918   -0.53092949  0.48584261  1.05658101\n",
      "  -0.50072836 -0.84319769]]\n",
      "V\n",
      " [[ 0.21586481  1.45565479 -0.19042048 -0.59953014  1.3611286  -0.6794735\n",
      "  -0.24616001 -0.26306417]\n",
      " [ 1.45317943  1.66992393  0.1658293   0.46269172 -0.29970686 -1.4233359\n",
      "  -0.18671738 -0.41701589]\n",
      " [-0.45523907 -0.94836636  0.04761672  0.45836321  0.19202407 -0.51034611\n",
      "  -1.05036504  0.16487293]\n",
      " [-0.07648394 -1.03627527  1.09574655  0.32055258 -0.16222928  0.71767422\n",
      "  -0.88820804  1.07659199]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\",q)\n",
    "print(\"K\\n\",k)\n",
    "print(\"V\\n\",v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.53022624, -2.33690479, -4.58513039,  2.17917594],\n",
       "       [ 1.30591489, -0.34623213,  5.86661238, -2.01576871],\n",
       "       [ 0.1867509 ,  0.90935888, -2.11340843,  1.39916299],\n",
       "       [-1.17779106, -1.98495452, -6.79276289,  0.0653833 ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to create a SELF ATTENTION MATRIX, we needd every word to look at every single other word - To see whether it has higher affinity towards it or not either by using any of the vector embedding technique\n",
    "# The above concept is represented by the Query(Q) - what i am looking for ?\n",
    "np.matmul(q,k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8679531007962409, 0.7981594453041116, 7.772860600917533)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# why do we need sqrt(d_k) in the denominator - In order to minimize the variance\n",
    "q.var(), k.var(), np.matmul(q,k.T).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8679531007962409, 0.7981594453041116, 0.9716075751146913)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q,k.T)/math.sqrt(d_k)\n",
    "q.var(),k.var(),scaled.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18746328, -0.82622061, -1.6210884 ,  0.77045504],\n",
       "       [ 0.46171064, -0.12241154,  2.0741607 , -0.71268186],\n",
       "       [ 0.06602641,  0.32150692, -0.74720272,  0.49467882],\n",
       "       [-0.41641202, -0.7017874 , -2.40160435,  0.02311649]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking\n",
    "* This is to ensure words don't get context from words generate in the future\n",
    "* This concept won't be used in the encoders, but this will be helpful in the decoders\n",
    "* So that we don't look at a future word while generating the current context\n",
    "### Why it is not required for encoders\n",
    "* As our inputs are passed simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why are we doing this so that \n",
    "# my can look at only itself \n",
    "# name can look at my name\n",
    "# is can look at my name is\n",
    "# vibhav can look at my name is vibhav\n",
    "mask = np.tril(np.ones((L,L)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why (-inf)\n",
    "# 1. Adding the matrices \n",
    "# 2. We are using the softmax function - so that all the values are squeezed between 0 to 1\n",
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18746328,        -inf,        -inf,        -inf],\n",
       "       [ 0.46171064, -0.12241154,        -inf,        -inf],\n",
       "       [ 0.06602641,  0.32150692, -0.74720272,        -inf],\n",
       "       [-0.41641202, -0.7017874 , -2.40160435,  0.02311649]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anyways we are not going to consider any context from it - so we replace them to be (-inf)\n",
    "scaled + mask\n",
    "# This is because of the softmax operation which we will be going to perform next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "* This operation is used to convert vector into a prob distribution - so that their values add up to 1 and they are interpretable and stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  return (np.exp(x).T/np.sum(np.exp(x),axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22869393, 0.12073852, 0.05453046, 0.59603709],\n",
       "       [0.14531324, 0.08102595, 0.72875804, 0.04490276],\n",
       "       [0.23420887, 0.30238282, 0.10385381, 0.35955449],\n",
       "       [0.290608  , 0.21845934, 0.03991623, 0.45101644]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This shouldn't be the case - because each row adds to 1\n",
    "attention_diff = softmax(scaled)\n",
    "attention_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.64201537, 0.35798463, 0.        , 0.        ],\n",
       "       [0.3656968 , 0.4721445 , 0.1621587 , 0.        ],\n",
       "       [0.290608  , 0.21845934, 0.03991623, 0.45101644]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the right method - cause as you can see for the first row it is 1.\n",
    "# similarly for the next row first two columns add to 1\n",
    "# similarly for the next row first three columns add to 1\n",
    "# for the last row all the 4 columns add to 1\n",
    "attention = softmax(scaled + mask)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21586481,  1.45565479, -0.19042048, -0.59953014,  1.3611286 ,\n",
       "        -0.6794735 , -0.24616001, -0.26306417],\n",
       "       [ 0.65880443,  1.53235985, -0.06288854, -0.21927104,  0.76657503,\n",
       "        -0.94576481, -0.22488046, -0.31817652],\n",
       "       [ 0.69123078,  1.16698786,  0.0163807 ,  0.07353868,  0.3873938 ,\n",
       "        -1.00325857, -0.34850334, -0.2663579 ],\n",
       "       [ 0.32752571,  0.28260301,  0.47698963,  0.08972168,  0.2645779 ,\n",
       "        -0.20508967, -0.55484926,  0.32459222]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention, v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21586481,  1.45565479, -0.19042048, -0.59953014,  1.3611286 ,\n",
       "        -0.6794735 , -0.24616001, -0.26306417],\n",
       "       [ 1.45317943,  1.66992393,  0.1658293 ,  0.46269172, -0.29970686,\n",
       "        -1.4233359 , -0.18671738, -0.41701589],\n",
       "       [-0.45523907, -0.94836636,  0.04761672,  0.45836321,  0.19202407,\n",
       "        -0.51034611, -1.05036504,  0.16487293],\n",
       "       [-0.07648394, -1.03627527,  1.09574655,  0.32055258, -0.16222928,\n",
       "         0.71767422, -0.88820804,  1.07659199]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  return (np.exp(x).T/np.sum(np.exp(x),axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q,k,v,mask=None):\n",
    "  d_k = q.shape[-1]\n",
    "  scaled = np.matmul(q,k.T)/math.sqrt(d_k)\n",
    "  if mask is not None:\n",
    "    scaled = scaled  + mask\n",
    "  attention = softmax(scaled)\n",
    "  out = np.matmul(attention,v)\n",
    "  return out,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.2751041   0.4958448   1.51066295 -0.14032124 -1.70178345  1.27926355\n",
      "   0.17624542 -0.59921872]\n",
      " [-0.28972129 -0.36650107  0.06336092 -0.77934791  0.03204285 -2.38194955\n",
      "   0.29029974  0.09106863]\n",
      " [-0.33995996  0.65953047  0.83787033 -0.0877142   2.24189994 -1.25214464\n",
      "  -1.11616416  0.3537038 ]\n",
      " [-0.25124162  1.18038437  1.01302541  1.19042449  0.25393742 -0.21432398\n",
      "   0.81623457  0.37074504]]\n",
      "K\n",
      " [[-0.83647365  0.22651162 -0.53878076 -0.08790401 -0.89791675 -0.61861813\n",
      "  -1.21527165  0.22252373]\n",
      " [-1.45747415 -1.1259484  -0.48800815 -0.1256447   0.67235244  0.46567906\n",
      "  -0.558623    0.01859098]\n",
      " [ 1.10837112 -0.8465118  -2.74098825 -1.44178702 -1.36898869 -2.25308163\n",
      "  -1.23695635 -0.39828107]\n",
      " [-1.16720464  0.27535859  0.891918   -0.53092949  0.48584261  1.05658101\n",
      "  -0.50072836 -0.84319769]]\n",
      "V\n",
      " [[ 0.21586481  1.45565479 -0.19042048 -0.59953014  1.3611286  -0.6794735\n",
      "  -0.24616001 -0.26306417]\n",
      " [ 1.45317943  1.66992393  0.1658293   0.46269172 -0.29970686 -1.4233359\n",
      "  -0.18671738 -0.41701589]\n",
      " [-0.45523907 -0.94836636  0.04761672  0.45836321  0.19202407 -0.51034611\n",
      "  -1.05036504  0.16487293]\n",
      " [-0.07648394 -1.03627527  1.09574655  0.32055258 -0.16222928  0.71767422\n",
      "  -0.88820804  1.07659199]]\n",
      "New V\n",
      " [[ 0.15441005 -0.13484978  0.63217612  0.13481179  0.18887218  0.07268811\n",
      "  -0.6655211   0.54016829]\n",
      " [-0.18608021 -0.39082813  0.06966897  0.29879994  0.30616052 -0.55375693\n",
      "  -0.85624423  0.0964786 ]\n",
      " [ 0.4151955   0.3747947   0.40447154  0.16235363  0.18977435 -0.38448945\n",
      "  -0.54255659  0.21650577]\n",
      " [ 0.32752571  0.28260301  0.47698963  0.08972168  0.2645779  -0.20508967\n",
      "  -0.55484926  0.32459222]]\n",
      "Attention\n",
      " [[0.22869393 0.12073852 0.05453046 0.59603709]\n",
      " [0.14531324 0.08102595 0.72875804 0.04490276]\n",
      " [0.23420887 0.30238282 0.10385381 0.35955449]\n",
      " [0.290608   0.21845934 0.03991623 0.45101644]]\n"
     ]
    }
   ],
   "source": [
    "# This is for the Encoder\n",
    "values,attention = scaled_dot_product_attention(q,k,v,mask=None)\n",
    "print(\"Q\\n\",q)\n",
    "print(\"K\\n\",k)\n",
    "print(\"V\\n\",v)\n",
    "print(\"New V\\n\",values)\n",
    "print(\"Attention\\n\",attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.2751041   0.4958448   1.51066295 -0.14032124 -1.70178345  1.27926355\n",
      "   0.17624542 -0.59921872]\n",
      " [-0.28972129 -0.36650107  0.06336092 -0.77934791  0.03204285 -2.38194955\n",
      "   0.29029974  0.09106863]\n",
      " [-0.33995996  0.65953047  0.83787033 -0.0877142   2.24189994 -1.25214464\n",
      "  -1.11616416  0.3537038 ]\n",
      " [-0.25124162  1.18038437  1.01302541  1.19042449  0.25393742 -0.21432398\n",
      "   0.81623457  0.37074504]]\n",
      "K\n",
      " [[-0.83647365  0.22651162 -0.53878076 -0.08790401 -0.89791675 -0.61861813\n",
      "  -1.21527165  0.22252373]\n",
      " [-1.45747415 -1.1259484  -0.48800815 -0.1256447   0.67235244  0.46567906\n",
      "  -0.558623    0.01859098]\n",
      " [ 1.10837112 -0.8465118  -2.74098825 -1.44178702 -1.36898869 -2.25308163\n",
      "  -1.23695635 -0.39828107]\n",
      " [-1.16720464  0.27535859  0.891918   -0.53092949  0.48584261  1.05658101\n",
      "  -0.50072836 -0.84319769]]\n",
      "V\n",
      " [[ 0.21586481  1.45565479 -0.19042048 -0.59953014  1.3611286  -0.6794735\n",
      "  -0.24616001 -0.26306417]\n",
      " [ 1.45317943  1.66992393  0.1658293   0.46269172 -0.29970686 -1.4233359\n",
      "  -0.18671738 -0.41701589]\n",
      " [-0.45523907 -0.94836636  0.04761672  0.45836321  0.19202407 -0.51034611\n",
      "  -1.05036504  0.16487293]\n",
      " [-0.07648394 -1.03627527  1.09574655  0.32055258 -0.16222928  0.71767422\n",
      "  -0.88820804  1.07659199]]\n",
      "New V\n",
      " [[ 0.21586481  1.45565479 -0.19042048 -0.59953014  1.3611286  -0.6794735\n",
      "  -0.24616001 -0.26306417]\n",
      " [ 0.65880443  1.53235985 -0.06288854 -0.21927104  0.76657503 -0.94576481\n",
      "  -0.22488046 -0.31817652]\n",
      " [ 0.69123078  1.16698786  0.0163807   0.07353868  0.3873938  -1.00325857\n",
      "  -0.34850334 -0.2663579 ]\n",
      " [ 0.32752571  0.28260301  0.47698963  0.08972168  0.2645779  -0.20508967\n",
      "  -0.55484926  0.32459222]]\n",
      "Attention\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.64201537 0.35798463 0.         0.        ]\n",
      " [0.3656968  0.4721445  0.1621587  0.        ]\n",
      " [0.290608   0.21845934 0.03991623 0.45101644]]\n"
     ]
    }
   ],
   "source": [
    "# This is for the Decoder\n",
    "values,attention = scaled_dot_product_attention(q,k,v,mask=mask)\n",
    "print(\"Q\\n\",q)\n",
    "print(\"K\\n\",k)\n",
    "print(\"V\\n\",v)\n",
    "print(\"New V\\n\",values)\n",
    "print(\"Attention\\n\",attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
